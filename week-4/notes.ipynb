{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38b5fcd",
   "metadata": {},
   "source": [
    "Current models are decoder only cuz they can be parallelized easily.\n",
    "\n",
    "# Finetuning\n",
    "\n",
    "Traditionally, ML models are trained for a specific use case and they perform poorly in other tasks. It is task-specific. The foundational (pre-trained) models for LLMs are trained using self-supervised learning using huge amounts of texts. This model can be retrained for a specific task, fine tune it. This fine-tuned model should be better at that task. There are different approaches to fine-tuning.\n",
    "\n",
    "## Prompting vs Fine tuning\n",
    "\n",
    "When prompting the model gets the prompt and creates an output, but the parameters are not tweaked. When finetuning you get in the end an updated model.\n",
    "\n",
    "## Why fine tuning?\n",
    "\n",
    "We want to **improve the performance** of the model in some way. Maybe we wat to adapt the model to a different domain or a different distribution of data (maybe you have documents that are not public and are structured in a certain way). Finetuning also **requires less data** than training a model from scratch, rarely makes sense to train from scratch. It's also more **efficient**, as the knowledge is updated from the one already present.\n",
    "\n",
    "A pre-trained model may be good at generating text but not at classifying stuff.\n",
    "\n",
    "## How\n",
    "\n",
    "First we have to choose a pre-trained model and get the data to fine tune it. For this last part usually a dataset is prepared using data relevant to the specific task. This may well be the most time consuming part. Then we get into the iterative loop of (re)training and evaluating until the result is satisfactory.\n",
    "\n",
    "Key parameters:\n",
    "\n",
    " * Controls how much the models parameters are adjusted during each training step (epoch).\n",
    "\n",
    " * Number of epochs: The number of times that the model sees the training set.\n",
    "\n",
    " ## Fine tuning methods\n",
    "\n",
    " **Supervised fine tuning** is the basic one and it's based on input-output pairs. Used for classification, Question answering and MT with labeled data. Simple and effective but it's data intensive. Also the end result is limited by the quality of the data.\n",
    "\n",
    " **Instruction tuning** is an supervised method. The model is provided instructions and an expected results. It's a type of supervised finetuning.\n",
    "\n",
    " Constructing the datasets is very time consuming and important, probably the most important part of the finetuning process is getting and processing the data.\n",
    "\n",
    " **Reinforcement learning from human feedback** is a combination of the supervised one with reinforcement learning where the model gets punished or rewarded based on the human feedback of its outputs. This is good when the evaluation metrics are blurry and hard to define like creative stuff. Human feedback collection can be expensive or inefficient, requires careful design of the feedback and can be computationally expensive. Aligns the model behavior to the human preferences.\n",
    "\n",
    " **Direct preferences optimization** is a process where, instead of having the reward and the finetuning separated, you have a dataset with the preferred output given previously (chosen and rejected). Maybe more sample efficient than the previous one but it's a very modern approach and the result can depend on quality and diversity of the human choices.\n",
    "\n",
    " ## Parameter efficient fine tuning\n",
    "\n",
    " We want to reduce the cost while maintaining performance. This is not easy, as there's a huge amount of parameters. The idea is not to require to load all the parameters into the GPU, instead you train them in a modular way. Note that finetuning generally makes the model worse for general purposes and better for the specictally trained ones.\n",
    "\n",
    " **LoRA** (Low Rank Adaptation)\n",
    "\n",
    " **Prefix turning**: Adds trainable prefix to the input sequence, guiding adaptation without losing core parameters\n",
    "\n",
    " Quantization addresses the huge amount of storage required for parameters by reducing the precision of the model. The idea is to use fewer bits instead of the standard 32 bit float. This means reduced model size and Faster inference but performance might be lost (but minimal unless precision is a huge factor)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
