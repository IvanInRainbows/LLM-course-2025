{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a49a61",
   "metadata": {},
   "source": [
    "# LLM\n",
    "\n",
    "We'll be focusing on transformers. Language models estimate the probability distribution of the sequence of words though the statistical patterns of languages to predict the next words (or tokens). They generate text based on an existing text. Different LMs may attach the task in different ways but all of them are based on predicting text. Some popular language modeling alorithms:\n",
    "\n",
    "* N-gram models: Predict next word based on the sequence of previous words.\n",
    "\n",
    "* Hidden markov models (HMM): Based of hidden states and transition probabilities. There's the probability of a state showing up and a probability of one state transitioning to another.\n",
    "\n",
    "* Neural Language models: In our case feed forward NNs (Neural Networks). Predict the probability of a word given a context.\n",
    "\n",
    "* Word embeddings: Different algorithms like Word2Vec and Glove to capture the semantics and enable arithmetic operations between words.\n",
    "\n",
    "* Recurrent NNs: \n",
    "\n",
    "* **Transformes**: Uses self attention to give weights for the importance of certain words. Originally designed for machine translation. This architecture:\n",
    "\n",
    "    * Captures logn range dependencies: Words build relations over distances, it's no longer necessary that the words are next to each other (As happened in Ngrams).\n",
    "\n",
    "    * Parallelize: This means faster processing but also inference.\n",
    "\n",
    "    * Great results: No explanation needed\n",
    "\n",
    "## Core components of the transformes architecture:\n",
    "\n",
    "Attention means that every word in the input attend to every other word, which maps the relative importance of each word with the rest. Multi head attention means having different attention systems that focus on different aspects. Attention doesn't care about the order of the words. In the end it's based on $W \\times W$. Thus the system receives the information of the position of each word in sequence and it's added to the embedding.\n",
    "\n",
    "There are encoder and decoder models.\n",
    "\n",
    "Feed forward Networks, layer normalization and residual connections.\n",
    "\n",
    "There are encoding-only models, which only get the encoder part of the model. This is used for representing the input and capture contextual relationships.\n",
    "\n",
    "The decoder only transformers are the generative models and they only use the decoder part. They specialize in generating text, predicting words.\n",
    "\n",
    "The encoder-decoder models are used in machine translation, dialogue systems, summarization. These combine the the capabilities of understanding and generating text.\n",
    "\n",
    "## Decoder only LLMs\n",
    "\n",
    "These are the generative AI models. Here we find the GPT family, which were the first ones that were pre-trained. Pre-training assumes that the data distribution stays relatively similar, thus needing to fine tune for specific tasks or domains. GPT-3 could be considered the fist LLM. Other decoder only models are LLaMA or Mistral. The architecture of the models might be designed following intuition and by trial and error.\n",
    "\n",
    "## Mixture of experts\n",
    "\n",
    "This means that certain networks specialize in certain tasks, then when the model is used only part of it is used. It's more modular and you dont have to compute that much stuff."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
